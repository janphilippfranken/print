{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "\n",
    "\n",
    "from improve import improve_algorithm\n",
    "from helpers import extract_code, format_response, insert_print_statement, delete_print_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'print.chat_models'; 'print' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/iphilipp/Documents/research/print/experiments/print-poc.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/iphilipp/Documents/research/print/experiments/print-poc.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mprint\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchat_models\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcrfm_chat_llm\u001b[39;00m \u001b[39mimport\u001b[39;00m crfmChatLLM\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/iphilipp/Documents/research/print/experiments/print-poc.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m llm \u001b[39m=\u001b[39m crfmChatLLM(model_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mopenai/gpt-4-0613\u001b[39m\u001b[39m\"\u001b[39m, crfm_api_key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mp4z0j9adj6edJOWBMnEqfPBZxAXlfOGd\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'print.chat_models'; 'print' is not a package"
     ]
    }
   ],
   "source": [
    "from print.chat_models.crfm_chat_llm import crfmChatLLM\n",
    "llm = crfmChatLLM(model_name=\"openai/gpt-4-0613\", crfm_api_key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility_with_feedback(algorithm_str: str):\n",
    "    \"Evaluates the informativeness of print statements in an algorithm and returns the print feedback.\"\n",
    "    \n",
    "    # task: compute element-wise sum across two lists \n",
    "    list1 = [2, 3, 4, 2, 1]\n",
    "    list2 = [5, 4, 3, 2, 1]\n",
    "    expected_output = [7, 7, 7, 4, 2]\n",
    "    \n",
    "    # redirect stdout to capture print statements\n",
    "    old_stdout = sys.stdout\n",
    "    new_stdout = io.StringIO()\n",
    "    sys.stdout = new_stdout\n",
    "    \n",
    "    result = None\n",
    "    try:\n",
    "        exec(algorithm_str, globals())\n",
    "        result = algorithm(list1, list2)  # this will run the algorithm defined in algorithm_str\n",
    "        score = np.sum([r == e for r, e in zip(result, expected_output)]) / len(expected_output) # this is the actual utility score, higher is better \n",
    "    except Exception as e:\n",
    "        sys.stdout = old_stdout\n",
    "        return (f\"Error in execution: {e}\", \"\")\n",
    "\n",
    "    # Reset stdout and get feedback\n",
    "    feedback = new_stdout.getvalue()\n",
    "    sys.stdout = old_stdout\n",
    "    \n",
    "    return (score, feedback)\n",
    "\n",
    "utility_str = \"\"\"def utility_with_feedback(algorithm_str: str):\n",
    "    \"Evaluates the informativeness of print statements in an algorithm and returns the print feedback.\"\n",
    "    \n",
    "    # task: compute element-wise sum across two lists \n",
    "    list1 = [2, 3, 4, 2, 1]\n",
    "    list2 = [5, 4, 3, 2, 1]\n",
    "    expected_output = [7, 7, 7, 4, 2]\n",
    "    \n",
    "    # redirect stdout to capture print statements\n",
    "    old_stdout = sys.stdout\n",
    "    new_stdout = io.StringIO()\n",
    "    sys.stdout = new_stdout\n",
    "    \n",
    "    result = None\n",
    "    try:\n",
    "        exec(algorithm_str, globals())\n",
    "        result = algorithm(list1, list2)  # this will run the algorithm defined in algorithm_str\n",
    "        score = np.sum([r == e for r, e in zip(result, expected_output)]) / len(expected_output) # this is the actual utility score, higher is better \n",
    "    except Exception as e:\n",
    "        sys.stdout = old_stdout\n",
    "        return (f\"Error in execution: {{e}}\", \"\")\n",
    "\n",
    "    # Reset stdout and get feedback\n",
    "    feedback = new_stdout.getvalue()\n",
    "    sys.stdout = old_stdout\n",
    "    \n",
    "    return (score, feedback)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_solution = \"\"\"\n",
    "def algorithm(list1, list2):\n",
    "    print(list1)\n",
    "    result = []\n",
    "    for i in range(len(list1)):\n",
    "        sum_value = list1[i] + list2[i] * 2\n",
    "        result.append(sum_value)\n",
    "    return result\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'Decide where and what to print in the given solution to help with debugging. You can only insert one print statement and only print one variable using the print statement.'}, {'role': 'user', 'content': 'Choose a new location and content (i.e., variable to print) for a print statement in the following solution to assist with debugging:\\n\\n```python\\n\\ndef algorithm(list1, list2):\\n    print(list1)\\n    result = []\\n    for i in range(len(list1)):\\n        sum_value = list1[i] + list2[i] * 2\\n        result.append(sum_value)\\n    return result\\n```\\n\\nYou will use the return of the print statement to improve the above solution. Upon improvement, you will be evaluated based on the following utility:\\n\\n```python\\ndef utility_with_feedback(algorithm_str: str):\\n    \"Evaluates the informativeness of print statements in an algorithm and returns the print feedback.\"\\n    \\n    # task: compute element-wise sum across two lists \\n    list1 = [2, 3, 4, 2, 1]\\n    list2 = [5, 4, 3, 2, 1]\\n    expected_output = [7, 7, 7, 4, 2]\\n    \\n    # redirect stdout to capture print statements\\n    old_stdout = sys.stdout\\n    new_stdout = io.StringIO()\\n    sys.stdout = new_stdout\\n    \\n    result = None\\n    try:\\n        exec(algorithm_str, globals())\\n        result = algorithm(list1, list2)  # this will run the algorithm defined in algorithm_str\\n        score = np.sum([r == e for r, e in zip(result, expected_output)]) / len(expected_output) # this is the actual utility score, higher is better \\n    except Exception as e:\\n        sys.stdout = old_stdout\\n        return (f\"Error in execution: {e}\", \"\")\\n\\n    # Reset stdout and get feedback\\n    feedback = new_stdout.getvalue()\\n    sys.stdout = old_stdout\\n    \\n    return (score, feedback)\\n```\\n\\nResponse format:\\nLine: <int>\\nVariable: <variable_name>\\n'}]\n",
      "Chosen Line: 6, Variable: sum_value\n",
      "\n",
      "def algorithm(list1, list2):\n",
      "    result = []\n",
      "    for i in range(len(list1)):\n",
      "        sum_value = list1[i] + list2[i] * 2\n",
      "        result.append(sum_value)\n",
      "    print(sum_value)\n",
      "    return result\n",
      "\n",
      "Score: 0.0\n",
      "Feedback:\n",
      "3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Choose the print statement's location and content\n",
    "line, variable = choose_print_statement(initial_solution, utility_str, llm)\n",
    "print(f\"Chosen Line: {line}, Variable: {variable}\")\n",
    "\n",
    "# Delete the existing print statement\n",
    "initial_solution = delete_print_statement(initial_solution)\n",
    "\n",
    "# Insert the new print statement\n",
    "modified_solution = insert_print_statement(initial_solution, line, variable)\n",
    "print(modified_solution)\n",
    "\n",
    "\n",
    "# Evaluate the modified algorithm using the utility_with_feedback function\n",
    "score, feedback = utility_with_feedback(modified_solution)\n",
    "print(f\"\\nScore: {score}\")\n",
    "print(f\"Feedback:\\n{feedback}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are an expert computer science researcher and programmer, especially skilled at optimizing algorithms.'}, {'role': 'user', 'content': 'Improve the following solution:\\n\\n```python\\nfrom sklearn.linear_model import LogisticRegression\\n\\ndef algorithm(train_samples, train_parity, test_samples):\\n    return 100\\n```\\n\\nYou will be evaluated based on this score function:\\n\\n```python\\nimport random\\nimport numpy as np\\nimport time\\n\\ndef utility(algorithm_str: str):\\n    \"Implements the parity learning task. Returns the number of correct predictions.\"\\n    \\n    n_tests = 1\\n    average_correct = 0\\n\\n    try:\\n        exec(algorithm_str, globals())\\n    except:\\n        return 0\\n    \\n    for _ in range(n_tests):\\n        start_time = time.time()\\n        n_bits = 10\\n        p_true = 0.3\\n        n_train_samples = 100\\n        n_test_samples = 20\\n        noise_level = 0.05\\n        true_bits = np.random.binomial(1, p_true, n_bits)\\n\\n        samples = np.random.binomial(1, 0.5, (n_train_samples + n_test_samples, n_bits))\\n        masked_samples = samples * true_bits\\n        parity = np.sum(masked_samples, axis=1) \\n        train_samples = samples[:n_train_samples]\\n        train_parity = parity[:n_train_samples]\\n        parity_noise = np.random.binomial(1, noise_level, n_train_samples)\\n        train_parity = (train_parity + parity_noise) \\n\\n        test_samples = samples[n_train_samples:]\\n        test_parity = parity[n_train_samples:]\\n\\n        # Because algorithm is a string, we can’t call it directly. Instead, we can use eval to evaluate it as a Python expression.\\n        try:\\n            predictions = algorithm(train_samples, train_parity, test_samples)\\n            test_parity = np.array(test_parity).reshape(-1)\\n            predictions = np.array(predictions).reshape(-1)\\n            correct = np.sum(predictions == test_parity) / n_test_samples\\n        except:\\n            correct = 0\\n        if time.time() - start_time > 0.1:\\n            return 0\\n        average_correct += correct / n_tests\\n\\n    return average_correct\\n\\n```\\n\\nYou must return an improved solution. Be as creative as you can under the constraints.\\nYour primary improvement must be novel and non-trivial. First, propose an idea, then implement it.'}]\n",
      "[{'role': 'system', 'content': 'You are an expert computer science researcher and programmer, especially skilled at optimizing algorithms.'}, {'role': 'user', 'content': 'Improve the following solution:\\n\\n```python\\nfrom sklearn.linear_model import LogisticRegression\\n\\ndef algorithm(train_samples, train_parity, test_samples):\\n    return 100\\n```\\n\\nYou will be evaluated based on this score function:\\n\\n```python\\nimport random\\nimport numpy as np\\nimport time\\n\\ndef utility(algorithm_str: str):\\n    \"Implements the parity learning task. Returns the number of correct predictions.\"\\n    \\n    n_tests = 1\\n    average_correct = 0\\n\\n    try:\\n        exec(algorithm_str, globals())\\n    except:\\n        return 0\\n    \\n    for _ in range(n_tests):\\n        start_time = time.time()\\n        n_bits = 10\\n        p_true = 0.3\\n        n_train_samples = 100\\n        n_test_samples = 20\\n        noise_level = 0.05\\n        true_bits = np.random.binomial(1, p_true, n_bits)\\n\\n        samples = np.random.binomial(1, 0.5, (n_train_samples + n_test_samples, n_bits))\\n        masked_samples = samples * true_bits\\n        parity = np.sum(masked_samples, axis=1) \\n        train_samples = samples[:n_train_samples]\\n        train_parity = parity[:n_train_samples]\\n        parity_noise = np.random.binomial(1, noise_level, n_train_samples)\\n        train_parity = (train_parity + parity_noise) \\n\\n        test_samples = samples[n_train_samples:]\\n        test_parity = parity[n_train_samples:]\\n\\n        # Because algorithm is a string, we can’t call it directly. Instead, we can use eval to evaluate it as a Python expression.\\n        try:\\n            predictions = algorithm(train_samples, train_parity, test_samples)\\n            test_parity = np.array(test_parity).reshape(-1)\\n            predictions = np.array(predictions).reshape(-1)\\n            correct = np.sum(predictions == test_parity) / n_test_samples\\n        except:\\n            correct = 0\\n        if time.time() - start_time > 0.1:\\n            return 0\\n        average_correct += correct / n_tests\\n\\n    return average_correct\\n\\n```\\n\\nYou must return an improved solution. Be as creative as you can under the constraints.\\nYour primary improvement must be novel and non-trivial. First, propose an idea, then implement it.'}]\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "def algorithm(train_samples, train_parity, test_samples):\n",
      "    model = LogisticRegression()\n",
      "    model.fit(train_samples, train_parity)\n",
      "    predictions = model.predict(test_samples)\n",
      "    return predictions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lpn import utility_class, initial_solution\n",
    "\n",
    "best_sol = improve_algorithm(initial_solution=initial_solution, utility=utility_class, language_model=llm)\n",
    "print(best_sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_str = \"\"\"def algorithm(train_samples, train_parity, test_samples):\n",
    "    model = LogisticRegression()\n",
    "    model.fit(train_samples, train_parity)\n",
    "    predictions = model.predict(test_samples)\n",
    "    return predictions\"\"\"\n",
    "\n",
    "from lpn import utility_class\n",
    "\n",
    "def meta_utility(improve_str: str):\n",
    "\n",
    "    \"\"\"\n",
    "    Evaluates the algorithm in improve_str to improve the algorithm in algorithm_str, \n",
    "    according to some downstream utility function. This meta-utility function can only be called n times.\n",
    "    \"\"\"\n",
    "    n_tests = 5\n",
    "    expected_utility = 0\n",
    "    for _ in range(n_tests):\n",
    "        try: \n",
    "            exec(improve_str, globals())\n",
    "        except:\n",
    "            continue\n",
    "        improved_algorithm_str = improve_algorithm(algorithm_str, utility_class, llm)\n",
    "        expected_utility += utility_class.fun(improved_algorithm_str) / n_tests\n",
    "    return expected_utility\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "print",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
