{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from lpn import utility_class\n",
    "\n",
    "BASE_URL = \"https://philipp.openai.azure.com/\"\n",
    "API_KEY = \"\"\n",
    "DEPLOYMENT_NAME = \"gpt-4\"\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_base=BASE_URL,\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_type=\"azure\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def extract_code(algorithm_str: str) -> str:\n",
    "    \"\"\"Extract code from algorithm string.\"\"\"\n",
    "    code = algorithm_str.split(\"```\")[1][6:]  # 6 is the length of \"python\"\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions [1 0 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 0]\n",
      "predictions [1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1]\n",
      "predictions [1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0]\n",
      "predictions [0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.575"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithm_str = \"\"\"def algorithm(train_samples, train_parity, test_samples):\n",
    "    return 100\"\"\"\n",
    "\n",
    "\n",
    "def meta_utility(improve_str: str):\n",
    "    \"\"\"\n",
    "    Evaluates the algorithm in improve_str to improve the algorithm in algorithm_str, \n",
    "    according to some downstream utility function. This meta-utility function can only be called n times.\n",
    "    \"\"\"\n",
    "    n_tests = 2\n",
    "    expected_utility = 0\n",
    "    for _ in range(n_tests):\n",
    "        try: \n",
    "            exec(improve_str, globals())\n",
    "        except:\n",
    "            continue\n",
    "        improved_algorithm_str = improve_algorithm(algorithm_str, utility_class, llm)\n",
    "        expected_utility += utility_class.func(improved_algorithm_str) / n_tests\n",
    "    return expected_utility\n",
    "\n",
    "improve_str = \"\"\"def improve_algorithm(initial_solution, utility, language_model, n_calls=1):\n",
    "\n",
    "    \\\"\\\"\\\"Improves a solution according to a utility function.\\\"\\\"\\\"\n",
    "    \n",
    "    expertise = \"You are an expert computer science researcher and programmer, especially skilled at optimizing algorithms.\"\n",
    "\n",
    "    message = f\\\"\\\"\\\"Improve the following solution:\n",
    "\n",
    "```python\n",
    "{initial_solution}\n",
    "```\n",
    "\n",
    "You will be evaluated based on this score function:\n",
    "\n",
    "```python\n",
    "{utility.str}\n",
    "```\n",
    "\n",
    "You must return an improved solution. Be as creative as you can under the constraints.\n",
    "Your primary improvement must be novel and non-trivial. First, propose an idea, then implement it. The solution algorithm function must have the name algorithm\\\"\\\"\\\"\n",
    "    \n",
    "    system_prompt = SystemMessagePromptTemplate.from_template(expertise)\n",
    "    human_prompt = HumanMessagePromptTemplate.from_template(message)\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "    chain = LLMChain(llm=language_model, prompt=chat_prompt)\n",
    "    new_solutions = []\n",
    "    for _ in range(n_calls):\n",
    "        new_solution = chain.run(stop=[\"System:\"])  \n",
    "        new_solution = extract_code(new_solution) \n",
    "        new_solutions.append(new_solution)\n",
    "    best_solution = max(new_solutions, key=lambda x: utility.func(x))\n",
    "    return best_solution\"\"\"\n",
    "\n",
    "expected_utility = meta_utility(improve_str)\n",
    "expected_utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_str = \"\"\"def meta_utility(improve_str: str):\n",
    "\n",
    "    \\\"\\\"\\\"\n",
    "    Evaluates the algorithm in improve_str to improve the algorithm in algorithm_str, \n",
    "    according to some downstream utility function. This meta-utility function can only be called n times.\n",
    "    \\\"\\\"\\\"\n",
    "    n_tests = 1\n",
    "    expected_utility = 0\n",
    "    for _ in range(n_tests):\n",
    "        try: \n",
    "            exec(improve_str, globals())\n",
    "        except:\n",
    "            continue\n",
    "        improved_algorithm_str = improve_algorithm(algorithm_str, utility_class, llm)\n",
    "        expected_utility += utility_class.func(improved_algorithm_str) / n_tests\n",
    "    return expected_utility\"\"\"\n",
    "\n",
    "\n",
    "class MetaUtility():\n",
    "\n",
    "    def __init__(self, ustr, ufunc):\n",
    "        self.str = ustr\n",
    "        self.func = ufunc\n",
    "\n",
    "meta_class = MetaUtility(ustr=meta_str, ufunc=meta_utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "improve_str = \"\"\"def improve_algorithm(initial_solution, utility, language_model, n_calls=1):\n",
    "\n",
    "    \\\"\\\"\\\"Improves a solution according to a utility function.\\\"\\\"\\\"\n",
    "    \n",
    "    expertise = \"You are an expert computer science researcher and programmer, especially skilled at optimizing algorithms.\"\n",
    "\n",
    "    message = f\\\"\\\"\\\"Improve the following solution:\n",
    "\n",
    "```python\n",
    "{{initial_solution}}\n",
    "```\n",
    "\n",
    "You will be evaluated based on this score function:\n",
    "\n",
    "```python\n",
    "{{utility.str}}\n",
    "```\n",
    "\n",
    "You must return an improved solution. Be as creative as you can under the constraints.\n",
    "Your primary improvement must be novel and non-trivial. First, propose an idea, then implement it. The solution algorithm function must have the name algorithm\\\"\\\"\\\"\n",
    "    \n",
    "    system_prompt = SystemMessagePromptTemplate.from_template(expertise)\n",
    "    human_prompt = HumanMessagePromptTemplate.from_template(message)\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "    chain = LLMChain(llm=language_model, prompt=chat_prompt)\n",
    "    new_solutions = []\n",
    "    for _ in range(n_calls):\n",
    "        new_solution = chain.run()  \n",
    "        new_solution = extract_code(new_solution) \n",
    "        new_solutions.append(new_solution)\n",
    "    best_solution = max(new_solutions, key=lambda x: utility.func(x))\n",
    "    return best_solution\"\"\"\n",
    "\n",
    "def improve_algorithm(initial_solution, utility, language_model, n_calls=1):\n",
    "\n",
    "    \"\"\"Improves a solution according to a utility function.\"\"\"\n",
    "    \n",
    "    expertise = \"You are an expert computer science researcher and programmer, especially skilled at optimizing algorithms. Do not user more than 5000 characters.\"\n",
    "\n",
    "    message = f\"\"\"Improve the following solution:\n",
    "\n",
    "```python\n",
    "{initial_solution}\n",
    "```\n",
    "\n",
    "You will be evaluated based on this score function:\n",
    "\n",
    "```python\n",
    "{utility.str}\n",
    "```\n",
    "\n",
    "You must return an improved solution. Be as creative as you can under the constraints.\n",
    "Your primary improvement must be novel and non-trivial. First, propose an idea, then implement it.\"\"\"\n",
    "    system_prompt = SystemMessagePromptTemplate.from_template(expertise)\n",
    "    human_prompt = HumanMessagePromptTemplate.from_template(message)\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "    chain = LLMChain(llm=language_model, prompt=chat_prompt)\n",
    "    new_solutions = []\n",
    "    for _ in range(n_calls):\n",
    "        print('running')\n",
    "        new_solution = chain.run(stop=['System:'])  \n",
    "        print(new_solution)\n",
    "\n",
    "        # new_solution = extract_code(new_solution) \n",
    "        # new_solutions.append(new_solution)\n",
    "    # best_solution = max(new_solutions, key=lambda x: utility.func(x))\n",
    "    # return best_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "The current code uses a language model to generate new solutions and then selects the best one according to some utility function. However, there are a couple of areas that can be improved:\n",
      "\n",
      "1. The same language model is used repeatedly. Since the language model is stochastic, it may not always yield the best solution on every run. It would be better to use multiple models and select the best solution across all of them.\n",
      "\n",
      "2. The current implementation does not take into account the performance of the generated solutions. If a solution performs poorly, it is still considered for the final selection. A better approach would be to discard solutions that fall below a certain threshold.\n",
      "\n",
      "Now, let's implement these improvements:\n",
      "\n",
      "```python\n",
      "def improve_algorithm(initial_solution, utility, language_models, n_calls=1, threshold=0.5):\n",
      "\n",
      "    \"\"\"Improves a solution according to a utility function.\"\"\"\n",
      "    \n",
      "    expertise = \"You are an expert computer science researcher and programmer, especially skilled at optimizing algorithms.\"\n",
      "\n",
      "    message = f\"\"\"Improve the following solution:\n",
      "\n",
      "```python\n",
      "{initial_solution}\n",
      "```\n",
      "\n",
      "You will be evaluated based on this score function:\n",
      "\n",
      "```python\n",
      "{utility.str}\n",
      "```\n",
      "\n",
      "You must return an improved solution. Be as creative as you can under the constraints.\n",
      "Your primary improvement must be novel and non-trivial. First, propose an idea, then implement it. The solution algorithm function must have the name algorithm\"\"\"\n",
      "    \n",
      "    system_prompt = SystemMessagePromptTemplate.from_template(expertise)\n",
      "    new_solutions = []\n",
      "    for language_model in language_models:\n",
      "        human_prompt = HumanMessagePromptTemplate.from_template(message)\n",
      "        chat_prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
      "        chain = LLMChain(llm=language_model, prompt=chat_prompt)\n",
      "        for _ in range(n_calls):\n",
      "            new_solution = chain.run()  \n",
      "            new_solution = extract_code(new_solution) \n",
      "            if utility.func(new_solution) >= threshold:\n",
      "                new_solutions.append(new_solution)\n",
      "\n",
      "    best_solution = max(new_solutions, key=lambda x: utility.func(x))\n",
      "    return best_solution\n",
      "```\n",
      "\n",
      "In this updated code, we take a list of language models as input and run each of them `n_calls` times. We then filter out solutions that have a utility score below some threshold. This way, we can ensure that we are not wasting computational resources on under-performing solutions. The function finally returns the best solution out of all the generated ones.\n"
     ]
    }
   ],
   "source": [
    "improved_improver = improve_algorithm(improve_str, meta_class, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_walk_solver(formula, max_iter, p):\n",
    "    n = max(abs(lit) for clause in formula for lit in clause)\n",
    "    assignments = [False] * (n + 1)\n",
    "    for _ in range(max_iter):\n",
    "        unsatisfied_clauses = [clause for clause in formula if not any(assignments[abs(lit)] == (lit > 0) for lit in clause)]\n",
    "        if not unsatisfied_clauses:\n",
    "            return assignments\n",
    "        clause_to_flip = random.choice(unsatisfied_clauses)\n",
    "        if random.random() < p:\n",
    "            lit_to_flip = random.choice(clause_to_flip)\n",
    "        else:\n",
    "            lit_to_flip = min(clause_to_flip, key=lambda lit: sum(assignments[abs(lit)] == (lit > 0) for clause in formula if lit in clause))\n",
    "        assignments[abs(lit_to_flip)] = not assignments[abs(lit_to_flip)]\n",
    "    return None\n",
    "\n",
    "def algorithm(formula):\n",
    "    return random_walk_solver(formula, max_iter=1000, p=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "print",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
