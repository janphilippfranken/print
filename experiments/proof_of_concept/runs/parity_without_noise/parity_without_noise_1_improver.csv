,model,improvements,cost,solutions,utility
0,improver,0,0.0,"import numpy as np

def algorithm(train_samples, train_parity, test_samples):
    n = train_samples.shape[1]
    aug_matrix = np.hstack((train_samples, train_parity.reshape(1, -1)))
    for i in range(n):
        pivot = np.where(aug_matrix[i:, i] == 1)[0]
        if len(pivot) > 0:
            pivot_row = pivot[0] + i
            if pivot_row != i:
                aug_matrix[[i, pivot_row]] = aug_matrix[[pivot_row, i]]
            for j in range(i + 1, len(aug_matrix)):
                if aug_matrix[j, i] == 1:
                    aug_matrix[j] = (aug_matrix[j] - aug_matrix[i]) % 2
    true_bits = np.zeros(n, dtype=int)
    for i in reversed(range(n)):
        true_bits[i] = aug_matrix[i, n]
        for j in range(i + 1, n):
            if aug_matrix[i, j] == 1:
                true_bits[i] ^= true_bits[j]
    test_parity = np.sum(true_bits + test_samples, axis=1) % 2
    return test_parity",0.0
1,improver,1,0.24036,def algorithm(*args): return 0,0.5499999999999999
2,improver,2,0.41486999999999996,"
def perceptron_train(samples, labels, epochs=10):
    w = np.zeros(samples.shape[1])
    for _ in range(epochs):
        for x, y in zip(samples, labels):
            y_pred = np.dot(w, x) % 2
            if y_pred != y:
                w += ((y - y_pred) * 2 - 1) * x
    return w

def perceptron_predict(samples, w):
    return np.dot(samples, w) % 2

def algorithm(train_samples, train_parity, test_samples):
    w = perceptron_train(train_samples, train_parity)
    return perceptron_predict(test_samples, w)
",0.6333333333333333
3,improver,3,0.65427,"

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

def mlp_train(samples, labels, epochs=10, learning_rate=0.1):
    w1 = np.random.rand(samples.shape[1], samples.shape[1])
    w2 = np.random.rand(samples.shape[1], 1)

    for _ in range(epochs):
        for x, y in zip(samples, labels):
            x = x.reshape(-1, 1)
            y = np.array([y])

            z1 = np.dot(w1.T, x)
            a1 = sigmoid(z1)
            z2 = np.dot(w2.T, a1)
            a2 = sigmoid(z2)

            delta2 = (a2 - y) * sigmoid_derivative(a2)
            delta1 = np.dot(w2, delta2) * sigmoid_derivative(a1)

            w2 -= learning_rate * np.dot(a1, delta2.T)
            w1 -= learning_rate * np.dot(x, delta1.T)

    return w1, w2

def mlp_predict(samples, w1, w2):
    z1 = np.dot(samples, w1)
    a1 = sigmoid(z1)
    z2 = np.dot(a1, w2)
    return np.round(sigmoid(z2))

def algorithm(train_samples, train_parity, test_samples):
    w1, w2 = mlp_train(train_samples, train_parity)
    return mlp_predict(test_samples, w1, w2)
",11.266666666666666
