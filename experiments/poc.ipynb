{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "\n",
    "\n",
    "from print.chat_models.crfm import crfmChatLLM\n",
    "\n",
    "from improve import improve_algorithm, choose_print_statement\n",
    "from helpers import extract_code, format_response, insert_print_statement, delete_print_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = crfmChatLLM(model_name=\"openai/gpt-4-0613\", crfm_api_key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility_with_feedback(algorithm_str: str):\n",
    "    \"Evaluates the informativeness of print statements in an algorithm and returns the print feedback.\"\n",
    "    \n",
    "    # task: compute element-wise sum across two lists \n",
    "    list1 = [2, 3, 4, 2, 1]\n",
    "    list2 = [5, 4, 3, 2, 1]\n",
    "    expected_output = [7, 7, 7, 4, 2]\n",
    "    \n",
    "    # redirect stdout to capture print statements\n",
    "    old_stdout = sys.stdout\n",
    "    new_stdout = io.StringIO()\n",
    "    sys.stdout = new_stdout\n",
    "    \n",
    "    result = None\n",
    "    try:\n",
    "        exec(algorithm_str, globals())\n",
    "        result = algorithm(list1, list2)  # this will run the algorithm defined in algorithm_str\n",
    "        score = np.sum([r == e for r, e in zip(result, expected_output)]) / len(expected_output) # this is the actual utility score, higher is better \n",
    "    except Exception as e:\n",
    "        sys.stdout = old_stdout\n",
    "        return (f\"Error in execution: {e}\", \"\")\n",
    "\n",
    "    # Reset stdout and get feedback\n",
    "    feedback = new_stdout.getvalue()\n",
    "    sys.stdout = old_stdout\n",
    "    \n",
    "    return (score, feedback)\n",
    "\n",
    "utility_str = \"\"\"def utility_with_feedback(algorithm_str: str):\n",
    "    \"Evaluates the informativeness of print statements in an algorithm and returns the print feedback.\"\n",
    "    \n",
    "    # task: compute element-wise sum across two lists \n",
    "    list1 = [2, 3, 4, 2, 1]\n",
    "    list2 = [5, 4, 3, 2, 1]\n",
    "    expected_output = [7, 7, 7, 4, 2]\n",
    "    \n",
    "    # redirect stdout to capture print statements\n",
    "    old_stdout = sys.stdout\n",
    "    new_stdout = io.StringIO()\n",
    "    sys.stdout = new_stdout\n",
    "    \n",
    "    result = None\n",
    "    try:\n",
    "        exec(algorithm_str, globals())\n",
    "        result = algorithm(list1, list2)  # this will run the algorithm defined in algorithm_str\n",
    "        score = np.sum([r == e for r, e in zip(result, expected_output)]) / len(expected_output) # this is the actual utility score, higher is better \n",
    "    except Exception as e:\n",
    "        sys.stdout = old_stdout\n",
    "        return (f\"Error in execution: {{e}}\", \"\")\n",
    "\n",
    "    # Reset stdout and get feedback\n",
    "    feedback = new_stdout.getvalue()\n",
    "    sys.stdout = old_stdout\n",
    "    \n",
    "    return (score, feedback)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_solution = \"\"\"\n",
    "def algorithm(list1, list2):\n",
    "    print(list1)\n",
    "    result = []\n",
    "    for i in range(len(list1)):\n",
    "        sum_value = list1[i] + list2[i] * 2\n",
    "        result.append(sum_value)\n",
    "    return result\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'Decide where and what to print in the given solution to help with debugging. You can only insert one print statement and only print one variable using the print statement.'}, {'role': 'user', 'content': 'Choose a new location and content (i.e., variable to print) for a print statement in the following solution to assist with debugging:\\n\\n```python\\n\\ndef algorithm(list1, list2):\\n    print(list1)\\n    result = []\\n    for i in range(len(list1)):\\n        sum_value = list1[i] + list2[i] * 2\\n        result.append(sum_value)\\n    return result\\n```\\n\\nYou will use the return of the print statement to improve the above solution. Upon improvement, you will be evaluated based on the following utility:\\n\\n```python\\ndef utility_with_feedback(algorithm_str: str):\\n    \"Evaluates the informativeness of print statements in an algorithm and returns the print feedback.\"\\n    \\n    # task: compute element-wise sum across two lists \\n    list1 = [2, 3, 4, 2, 1]\\n    list2 = [5, 4, 3, 2, 1]\\n    expected_output = [7, 7, 7, 4, 2]\\n    \\n    # redirect stdout to capture print statements\\n    old_stdout = sys.stdout\\n    new_stdout = io.StringIO()\\n    sys.stdout = new_stdout\\n    \\n    result = None\\n    try:\\n        exec(algorithm_str, globals())\\n        result = algorithm(list1, list2)  # this will run the algorithm defined in algorithm_str\\n        score = np.sum([r == e for r, e in zip(result, expected_output)]) / len(expected_output) # this is the actual utility score, higher is better \\n    except Exception as e:\\n        sys.stdout = old_stdout\\n        return (f\"Error in execution: {e}\", \"\")\\n\\n    # Reset stdout and get feedback\\n    feedback = new_stdout.getvalue()\\n    sys.stdout = old_stdout\\n    \\n    return (score, feedback)\\n```\\n\\nResponse format:\\nLine: <int>\\nVariable: <variable_name>\\n'}]\n",
      "Chosen Line: 6, Variable: sum_value\n",
      "\n",
      "def algorithm(list1, list2):\n",
      "    result = []\n",
      "    for i in range(len(list1)):\n",
      "        sum_value = list1[i] + list2[i] * 2\n",
      "        result.append(sum_value)\n",
      "    print(sum_value)\n",
      "    return result\n",
      "\n",
      "Score: 0.0\n",
      "Feedback:\n",
      "3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Choose the print statement's location and content\n",
    "line, variable = choose_print_statement(initial_solution, utility_str, llm)\n",
    "print(f\"Chosen Line: {line}, Variable: {variable}\")\n",
    "\n",
    "# Delete the existing print statement\n",
    "initial_solution = delete_print_statement(initial_solution)\n",
    "\n",
    "# Insert the new print statement\n",
    "modified_solution = insert_print_statement(initial_solution, line, variable)\n",
    "print(modified_solution)\n",
    "\n",
    "\n",
    "# Evaluate the modified algorithm using the utility_with_feedback function\n",
    "score, feedback = utility_with_feedback(modified_solution)\n",
    "print(f\"\\nScore: {score}\")\n",
    "print(f\"Feedback:\\n{feedback}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "print",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
